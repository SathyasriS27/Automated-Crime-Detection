{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wVTet-sDU9kT",
    "outputId": "7a1afa34-e14c-4dc0-dd51-aa49ff888f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdo-ICq615RJ",
    "outputId": "367548b0-a0c6-48f5-ae29-f7023484388f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-contrib-python) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjn5BlIo1ekW"
   },
   "source": [
    "# **Extract embeddings from face dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XN6NxT6A0FZ8"
   },
   "outputs": [],
   "source": [
    "# Importing all necessary packages\n",
    "\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2P3w76101_7J",
    "outputId": "988ed5b9-4b61-4c8d-bb2a-c1b0b5e862cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face detector...\n",
      "\n",
      "Loading face recognizer...\n"
     ]
    }
   ],
   "source": [
    "# Load face detector\n",
    "\n",
    "print(\"Loading face detector...\")\n",
    "\n",
    "protoPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'deploy.prototxt'])\n",
    "modelPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "\n",
    "\n",
    "# Load face recognizer\n",
    "\n",
    "print(\"\\nLoading face recognizer...\")\n",
    "embedder = cv2.dnn.readNetFromTorch('/content/drive/MyDrive/Open Lab/openface_nn4.small2.v1.t7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDk_iHf1OO2Q",
    "outputId": "9b77a285-4bd2-43fe-bb62-ea998bd41965"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Quantifying faces...\n"
     ]
    }
   ],
   "source": [
    "# Entering paths to our images dataset\n",
    "\n",
    "print('\\nQuantifying faces...')\n",
    "imagePaths = list(paths.list_images('/content/drive/MyDrive/Open Lab/Datasets'))\n",
    "\n",
    "knownEmbeddings = []\n",
    "knownNames = []\n",
    "\n",
    "# Total number of faces\n",
    "total = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "036MFBrCQFz6",
    "outputId": "9ae917bc-1217-44c2-e15f-8ee733a0c8c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image 1/25\n",
      "Processing image 2/25\n",
      "Processing image 3/25\n",
      "Processing image 4/25\n",
      "Processing image 5/25\n",
      "Processing image 6/25\n",
      "Processing image 7/25\n",
      "Processing image 8/25\n",
      "Processing image 9/25\n",
      "Processing image 10/25\n",
      "Processing image 11/25\n",
      "Processing image 12/25\n",
      "Processing image 13/25\n",
      "Processing image 14/25\n",
      "Processing image 15/25\n",
      "Processing image 16/25\n",
      "Processing image 17/25\n",
      "Processing image 18/25\n",
      "Processing image 19/25\n",
      "Processing image 20/25\n",
      "Processing image 21/25\n",
      "Processing image 22/25\n",
      "Processing image 23/25\n",
      "Processing image 24/25\n",
      "Processing image 25/25\n"
     ]
    }
   ],
   "source": [
    "# Loop over all image paths\n",
    "\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "  print(\"Processing image {}/{}\".format(i + 1, len(imagePaths)))                  # Extract the person name from the image path\n",
    "  name = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "  image = cv2.imread(imagePath)                                                   # Load the image\n",
    "  image = imutils.resize(image, width=600)                                        # Resize to (600, 600)\n",
    "  (h, w) = image.shape[:2]                                                        # Store image dimensions\n",
    "  imageBlob = cv2.dnn.blobFromImage(                                              # Construct a blob from the image\n",
    "  \tcv2.resize(image, (300, 300)), 1.0, (300, 300),\n",
    "  \t(104.0, 177.0, 123.0), swapRB=False, crop=False)\n",
    "\n",
    "  detector.setInput(imageBlob)                                                    # Face Detector to localize face in an image\n",
    "  detections = detector.forward()       \n",
    "\n",
    "  if len(detections) > 0:                                                         # Ensure at least one face was found\n",
    "    i = np.argmax(detections[0, 0, :, 2])\n",
    "    confidence = detections[0, 0, i, 2]        \n",
    "\n",
    "    if (confidence > 0.1):                                                          # Filtering weak detections\n",
    "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\t\t\t\n",
    "      face = image[startY:endY, startX:endX]\n",
    "      (fH, fW) = face.shape[:2]\n",
    "\t\t\t\n",
    "      if (fW < 20 or fH < 20):\n",
    "        continue    \n",
    "\n",
    "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), \n",
    "                                       swapRB=True, crop=False)                   # Create blob\n",
    "\n",
    "      embedder.setInput(faceBlob)\n",
    "      vec = embedder.forward()\n",
    "\n",
    "      knownNames.append(name)                                                     # Append name to list\n",
    "      knownEmbeddings.append(vec.flatten())                                       # Append flattened embedding to list\n",
    "      total += 1                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "glLTq5YeYOLZ",
    "outputId": "fee56187-0fe8-476b-fc39-201c9947d5af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serializing 25 encodings:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save (pickle) the embeddings and the names\n",
    "\n",
    "print ('Serializing {} encodings:\\n'.format(total))\n",
    "\n",
    "data = {'embeddings': knownEmbeddings, 'names': knownNames}                       # Saved as a dictionary/ HashMap\n",
    "f = open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'wb')\n",
    "f.write(pickle.dumps(data))                                                       # Stored as a ByteStream\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dlH_sXrZmx3"
   },
   "source": [
    "# **Train the face recognition model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EeL68fqZmUy"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "import argparse\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tQXtEKrgdXGy",
    "outputId": "070fd999-812a-49cf-f94a-fb3c4458b482"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face embeddings:\n",
      "\n",
      "Loaded.\n",
      "\n",
      "\n",
      "Encoding labels:\n",
      "\n",
      "\n",
      "Labels encoded.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading face embeddings:\\n\")\n",
    "data = pickle.loads(open('/content/drive/MyDrive/Open Lab/Output Embeddings/embeddings.pickle', 'rb').read())\n",
    "print(\"Loaded.\\n\\n\")\n",
    "\n",
    "print(\"Encoding labels:\\n\")\n",
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(data['names'])\n",
    "print(\"\\nLabels encoded.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QoQw-Auiolr",
    "outputId": "a7a89bf4-9574-4e06-9acf-f25e435446f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      "\n",
      "\n",
      "Model trained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training model...\\n\")\n",
    "recognizer = SVC(C = 1.0, kernel = \"linear\", probability = True)\n",
    "recognizer.fit(data[\"embeddings\"], labels)\n",
    "print(\"\\nModel trained.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6RnEopljrS2"
   },
   "outputs": [],
   "source": [
    "# Save the actual face recognition model\n",
    "f = open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"wb\")\n",
    "f.write(pickle.dumps(recognizer))\n",
    "f.close()\n",
    "\n",
    "# Save the label encoder\n",
    "f = open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"wb\")\n",
    "f.write(pickle.dumps(le))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmQiypWqk1MV"
   },
   "source": [
    "# **Recognise faces using OpenCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDVTA-esk6Wo"
   },
   "outputs": [],
   "source": [
    "from imutils.video import VideoStream\n",
    "from imutils.video import FPS\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KXSzYgIyk-za",
    "outputId": "47e69b48-28f4-4109-8c3b-79d09b794eff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading face detector...\n",
      "\n",
      "Loaded face detector.\n",
      "\n",
      "\n",
      "Loading face recognizer...\n",
      "\n",
      "Loaded Face Recognizer.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load our serialized face detector\n",
    "\n",
    "print(\"Loading face detector...\\n\")\n",
    "protoPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'deploy.prototxt'])\n",
    "modelPath = os.path.sep.join(['/content/drive/MyDrive/Open Lab/face_detection_model', 'res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "print(\"Loaded face detector.\\n\")\n",
    "\n",
    "# Load our serialized face embedding model\n",
    "print(\"\\nLoading face recognizer...\\n\")\n",
    "embedder = cv2.dnn.readNetFromTorch('/content/drive/MyDrive/Open Lab/openface_nn4.small2.v1.t7')\n",
    "print(\"Loaded Face Recognizer.\\n\")\n",
    "\n",
    "# Load the SVM Model and LabelEncoder\n",
    "recognizer = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/recognizer.pickle', \"rb\").read())\n",
    "le = pickle.loads(open('/content/drive/MyDrive/Open Lab/Trained Models/le.pickle', \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "ope1aUvrlJth",
    "outputId": "80c8ee5d-9ac2-4b57-84cb-da61fa6029a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting video stream...\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-bf59fa0f88aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imutils/convenience.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# grab the image size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# if both the width and height are None, then return the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# Initialize the video stream, then allow the camera sensor to warm up\n",
    "\n",
    "print(\"Starting video stream...\\n\")\n",
    "\n",
    "vs = VideoStream(src = 0).start()\n",
    "time.sleep(2.0)\n",
    "\n",
    "# Start FPS Throughput Estimator\n",
    "fps = FPS().start()\n",
    "\n",
    "# Loop over frames\n",
    "while True:\n",
    "  frame = vs.read()\n",
    "  frame = imutils.resize(frame, width = 600)\n",
    "  (h, w) = frame.shape[:2]\n",
    "\n",
    "  imageBlob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0), swapRB = False, crop = False)\n",
    "\n",
    "  detector.setInput(imageBlob)\n",
    "  detections = detector.forward()\n",
    "\n",
    "  for i in range(0, detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "\n",
    "    if (confidence > 0.2):\n",
    "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "      face = frame[startY: endY, startX: endX]\n",
    "      (fH, fW) = face.shape[:2]\n",
    "\n",
    "      if ((fw < 20) or (fH < 20)):\n",
    "        continue\n",
    "      \n",
    "      faceBlob = cv2.dnn.blobFromImage(face, 1.0/255, (96, 96), (0, 0, 0), swapRB = True, crop = False)\n",
    "\n",
    "      embedder.setInput(faceBlob)\n",
    "      vec = embedder.forward()\n",
    "\n",
    "      preds = recognizer.predict_proba(vec)[0]\n",
    "      j = np.argmax(preds)\n",
    "      proba = preds[j]\n",
    "      name = le.classes_[j]\n",
    "\n",
    "      text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "      y = startY - 10 if (startY - 10 > 10) else (startY + 10)\n",
    "\n",
    "      cv2.rectangle(frame, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "      cv2.putText(frame, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "  \n",
    "  fps.update()\n",
    "  cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "  key = cv2.waitKey(1) & 0xFF\n",
    "  if (key == ord('q') or key == ord('Q')):\n",
    "    break\n",
    "  \n",
    "fps.stop()\n",
    "\n",
    "print(\"Elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"Approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vs.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4nD-nBq5jKLE"
   },
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBWF79rjrRoD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPg/Qu9OKB1hOggeSyP7p7",
   "include_colab_link": true,
   "mount_file_id": "https://github.com/newb-dev-1008/Human-PokeDex/blob/master/Human_PokeDex.ipynb",
   "name": "Human PokeDex.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
