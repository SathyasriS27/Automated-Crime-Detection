{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHu1mDcHAcPU"
   },
   "source": [
    "# **Human PokeDex - Playground Notebook**\n",
    "\n",
    "This notebook will be used for trying parts of the software to test their implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rgxQ9qbApnx"
   },
   "source": [
    "# **Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3V9fH4sBNXr"
   },
   "source": [
    "## **Functions for augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-MlaMMJGyvv",
    "outputId": "6e41eb1e-67d2-4b48-f8e2-42a69fc062e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Augmentor\n",
      "  Downloading https://files.pythonhosted.org/packages/cb/79/861f38d5830cff631e30e33b127076bfef8ac98171e51daa06df0118c75f/Augmentor-0.2.8-py2.py3-none-any.whl\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (0.16.0)\n",
      "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (7.1.2)\n",
      "Requirement already satisfied: tqdm>=4.9.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (4.41.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from Augmentor) (1.19.5)\n",
      "Installing collected packages: Augmentor\n",
      "Successfully installed Augmentor-0.2.8\n"
     ]
    }
   ],
   "source": [
    "!pip install Augmentor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U795lUjnHr2K",
    "outputId": "b015e4e9-1f3a-487f-923d-7d274cf7a8a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: imutils in /usr/local/lib/python3.7/dist-packages (0.5.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV7B9ibC0-yl",
    "outputId": "05fb022a-058f-4aa5-8928-be50df4cff96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyrebase\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/1f/86ec68bfe0d6ffeadbab2ab0651ddf1c7206a0c4421b39baeb69b3a9c5cd/Pyrebase-3.0.27-py3-none-any.whl\n",
      "Collecting oauth2client==3.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c0/7b/bc893e35d6ca46a72faa4b9eaac25c687ce60e1fbe978993fe2de1b0ff0d/oauth2client-3.0.0.tar.gz (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.1MB/s \n",
      "\u001b[?25hCollecting gcloud==0.17.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/9d/56ecba02b4c117a43a906098095bccaea94d3edf57cb407c071efd87002f/gcloud-0.17.0.tar.gz (458kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 14.5MB/s \n",
      "\u001b[?25hCollecting pycryptodome==3.4.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/ef/4514a01be3d8f5383cd12fc5612ffd8a2508ac7e7ff6bde2da708e71c9a3/pycryptodome-3.4.3.tar.gz (6.5MB)\n",
      "\u001b[K     |████████████████████████████████| 6.5MB 9.8MB/s \n",
      "\u001b[?25hCollecting requests-toolbelt==0.7.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/57/60/cc85ca45c85585191e70e21687aeaa74ec4e555a1404628ba77b8af7d92e/requests_toolbelt-0.7.0-py2.py3-none-any.whl (52kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 6.2MB/s \n",
      "\u001b[?25hCollecting python-jwt==2.0.1\n",
      "  Downloading https://files.pythonhosted.org/packages/dd/2a/9c4230b09f63737e7beb34e3a19895cd50c5ff88af16d3cd54cd71e2325a/python_jwt-2.0.1-py2.py3-none-any.whl\n",
      "Collecting requests==2.11.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/03/92d3278bf8287c5caa07dbd9ea139027d5a3592b0f4d14abf072f890fab2/requests-2.11.1-py2.py3-none-any.whl (514kB)\n",
      "\u001b[K     |████████████████████████████████| 522kB 37.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client==3.0.0->pyrebase) (0.17.4)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client==3.0.0->pyrebase) (0.4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from oauth2client==3.0.0->pyrebase) (0.2.8)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from oauth2client==3.0.0->pyrebase) (4.7.2)\n",
      "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from oauth2client==3.0.0->pyrebase) (1.15.0)\n",
      "Requirement already satisfied: googleapis-common-protos in /usr/local/lib/python3.7/dist-packages (from gcloud==0.17.0->pyrebase) (1.53.0)\n",
      "Requirement already satisfied: protobuf!=3.0.0.b2.post1,>=3.0.0b2 in /usr/local/lib/python3.7/dist-packages (from gcloud==0.17.0->pyrebase) (3.12.4)\n",
      "Collecting jws>=0.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/01/9e/1536d578ed50f5fe8196310ddcc921a3cd8e973312d60ac74488b805d395/jws-0.1.3.tar.gz\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.0.0.b2.post1,>=3.0.0b2->gcloud==0.17.0->pyrebase) (54.2.0)\n",
      "Building wheels for collected packages: oauth2client, gcloud, pycryptodome, jws\n",
      "  Building wheel for oauth2client (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for oauth2client: filename=oauth2client-3.0.0-cp37-none-any.whl size=106383 sha256=9764b4a13332d320f4862d6f0aec535760b5fb4769e1062cd2c7c9fad3a6baaa\n",
      "  Stored in directory: /root/.cache/pip/wheels/48/f7/87/b932f09c6335dbcf45d916937105a372ab14f353a9ca431d7d\n",
      "  Building wheel for gcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gcloud: filename=gcloud-0.17.0-cp37-none-any.whl size=638015 sha256=59ec55764619ed7d83d58ecf361e353e25c3ebdac74e349ab102c29cf33f178b\n",
      "  Stored in directory: /root/.cache/pip/wheels/b9/04/ff/66c87c7e2419fd477f52015468ec5301d9480b6f3896babda4\n",
      "  Building wheel for pycryptodome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pycryptodome: filename=pycryptodome-3.4.3-cp37-cp37m-linux_x86_64.whl size=6813225 sha256=4ecaa18788928164d003af585804f3d51f4619096f7961785e123d439a73ccd0\n",
      "  Stored in directory: /root/.cache/pip/wheels/8f/9d/63/f6aea5612d1682a50b25857b0358e2ecec878a0bc42ba3af4c\n",
      "  Building wheel for jws (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for jws: filename=jws-0.1.3-cp37-none-any.whl size=9411 sha256=6b5c28cc0eebce5638ee1933ffc796651314210ee0ce85473339ef735ce7289c\n",
      "  Stored in directory: /root/.cache/pip/wheels/42/66/97/11aed97607cb2379c1d35d02bdc159bcc3918eb0cc20f7eb9a\n",
      "Successfully built oauth2client gcloud pycryptodome jws\n",
      "\u001b[31mERROR: tensorflow-datasets 4.0.1 has requirement requests>=2.19.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.4.1 has requirement requests<3,>=2.21.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: spacy 2.2.4 has requirement requests<3.0.0,>=2.13.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pydrive 1.3.1 has requirement oauth2client>=4.0.0, but you'll have oauth2client 3.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pandas-datareader 0.9.0 has requirement requests>=2.19.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-api-core 1.26.2 has requirement requests<3.0.0dev,>=2.18.0, but you'll have requests 2.11.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: oauth2client, gcloud, pycryptodome, requests, requests-toolbelt, jws, python-jwt, pyrebase\n",
      "  Found existing installation: oauth2client 4.1.3\n",
      "    Uninstalling oauth2client-4.1.3:\n",
      "      Successfully uninstalled oauth2client-4.1.3\n",
      "  Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "Successfully installed gcloud-0.17.0 jws-0.1.3 oauth2client-3.0.0 pycryptodome-3.4.3 pyrebase-3.0.27 python-jwt-2.0.1 requests-2.11.1 requests-toolbelt-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyrebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS3sVzOk6DTx",
    "outputId": "6d124654-552f-4489-b7a7-8e3ea234c3e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.11.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Khtxk9lCwvpE",
    "outputId": "addcffd4-e432-422d-fb6b-f17926dcb299"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting firebase-admin\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/4b/d4a8aa86379ad58bdd616e517fc3dfe8a6cffe1ae4f6a3db5a3e94cc979b/firebase_admin-4.5.3-py3-none-any.whl (111kB)\n",
      "\r",
      "\u001b[K     |███                             | 10kB 13.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 20kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 30kB 13.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 40kB 9.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 51kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 61kB 9.0MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 71kB 7.4MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 81kB 8.0MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▍     | 92kB 8.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 102kB 7.7MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 112kB 7.7MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: google-api-python-client>=1.7.8 in /usr/local/lib/python3.7/dist-packages (from firebase-admin) (1.12.8)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-firestore>=1.4.0; platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from firebase-admin) (1.7.0)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-storage>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from firebase-admin) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\" in /usr/local/lib/python3.7/dist-packages (from firebase-admin) (1.26.2)\n",
      "Requirement already satisfied, skipping upgrade: cachecontrol>=0.12.6 in /usr/local/lib/python3.7/dist-packages (from firebase-admin) (0.12.6)\n",
      "Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.0.4)\n",
      "Requirement already satisfied, skipping upgrade: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: google-auth>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (1.28.0)\n",
      "Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (3.0.1)\n",
      "Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.7.8->firebase-admin) (0.17.4)\n",
      "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from google-cloud-firestore>=1.4.0; platform_python_implementation != \"PyPy\"->firebase-admin) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.7/dist-packages (from google-cloud-firestore>=1.4.0; platform_python_implementation != \"PyPy\"->firebase-admin) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.7/dist-packages (from google-cloud-storage>=1.18.0->firebase-admin) (0.4.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (3.12.4)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (20.9)\n",
      "Collecting requests<3.0.0dev,>=2.18.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/c1/24814557f1d22c56d50280771a17307e6bf87b70727d975fd6b2ce6b014a/requests-2.25.1-py2.py3-none-any.whl (61kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 4.6MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (54.2.0)\n",
      "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (1.53.0)\n",
      "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.29.0; extra == \"grpc\" in /usr/local/lib/python3.7/dist-packages (from google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (1.32.0)\n",
      "Requirement already satisfied, skipping upgrade: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from cachecontrol>=0.12.6->firebase-admin) (1.0.2)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.7.8->firebase-admin) (0.2.8)\n",
      "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.7.8->firebase-admin) (4.2.1)\n",
      "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.16.0->google-api-python-client>=1.7.8->firebase-admin) (4.7.2)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (2020.12.5)\n",
      "Requirement already satisfied, skipping upgrade: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (2.10)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\"->firebase-admin) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.16.0->google-api-python-client>=1.7.8->firebase-admin) (0.4.8)\n",
      "\u001b[31mERROR: pyrebase 3.0.27 has requirement requests==2.11.1, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.25.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Installing collected packages: firebase-admin, requests\n",
      "  Found existing installation: firebase-admin 4.4.0\n",
      "    Uninstalling firebase-admin-4.4.0:\n",
      "      Successfully uninstalled firebase-admin-4.4.0\n",
      "  Found existing installation: requests 2.11.1\n",
      "    Uninstalling requests-2.11.1:\n",
      "      Successfully uninstalled requests-2.11.1\n",
      "Successfully installed firebase-admin-4.5.3 requests-2.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade firebase-admin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJDEv1J3Bc1o"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from imutils import paths as pts\n",
    "import Augmentor\n",
    "import cv2\n",
    "import os\n",
    "import firebase_admin\n",
    "from firebase_admin import firestore\n",
    "from firebase_admin import credentials\n",
    "from skimage import io\n",
    "import gdown\n",
    "import requests\n",
    "import pyrebase\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# ----------------------------- Functions -----------------------------\n",
    "\n",
    "# Function to extract path of new datasets uploaded\n",
    "def extractPath(i):\n",
    "  a = (str(i).split(',')[1:])\n",
    "  b = str(a[-1]).rstrip('>')\n",
    "  c = b.lstrip()\n",
    "  return c\n",
    "\n",
    "# Function to retrieve person's name\n",
    "def firestoreName():\n",
    "  users_ref = db.collection(u'New Users')\n",
    "  docs = users_ref.stream()\n",
    "\n",
    "  userName = \"\"\n",
    "\n",
    "  for doc in docs:\n",
    "    dict1 = doc.to_dict()\n",
    "    userName = dict1['Username']\n",
    "  \n",
    "  return userName\n",
    "\n",
    "# Function to blur the image\n",
    "def blurImage(imagePath, userName, i):\n",
    "    old_image = io.imread(imagePath)\n",
    "    # old_image = cv2.imread(imagePath)\n",
    "    image = old_image.copy()\n",
    "    blurredImage = cv2.blur(image, (3, 3))\n",
    "    filename = \"blurred_\" + userName + \"_\" + str(i) + \".jpg\"\n",
    "    destination = 'Datasets/' + userName + '/' + filename\n",
    "    # os.chdir(r'/content/drive/MyDrive/Open Lab/Datasets 1/' + userName)\n",
    "    cv2.imwrite(filename, blurredImage) \n",
    "    return destination, filename\n",
    "\n",
    "# Function to sharpen the image\n",
    "def sharpenImage(imagePath, userName, i):\n",
    "    old_image = io.imread(imagePath)\n",
    "    image = old_image.copy()\n",
    "    kernel = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
    "    sharpImage = cv2.filter2D(image, -1, kernel)\n",
    "    filename = \"sharpened_\" + userName + \"_\" + str(i) + \".jpg\"\n",
    "    destination = 'Datasets/' + userName + '/' + filename\n",
    "    # os.chdir(r'/content/drive/MyDrive/Open Lab/Datasets 1/' + userName)\n",
    "    cv2.imwrite(filename, sharpImage) \n",
    "    return destination, filename\n",
    "\n",
    "# Function to add Sepia effect \n",
    "def sepiaImage(imagePath, userName, i):\n",
    "    old_image = io.imread(imagePath)\n",
    "    image = old_image.copy()\n",
    "    kernel = np.array([[0.272, 0.534, 0.131], [0.349, 0.686, 0.168], [0.393, 0.769, 0.189]])\n",
    "    sepiaImage = cv2.filter2D(image, -1, kernel)\n",
    "    filename = \"sepia_\" + userName +  \"_\" + str(i) + \".jpg\"\n",
    "    destination = 'Datasets/' + userName + '/' + filename\n",
    "    # os.chdir(r'/content/drive/MyDrive/Open Lab/Datasets 1/' + userName)\n",
    "    cv2.imwrite(filename, sepiaImage) \n",
    "    return destination, filename\n",
    "\n",
    "# Function to add brightness\n",
    "def brightImage(imagePath, userName, i):\n",
    "    old_image = io.imread(imagePath)\n",
    "    image = old_image.copy()\n",
    "    brightImage = cv2.convertScaleAbs(image, 3)\n",
    "    filename = \"bright_\" + userName +  \"_\" + str(i) + \".jpg\"\n",
    "    destination = 'Datasets/' + userName + '/' + filename\n",
    "    cv2.imwrite(filename, brightImage) \n",
    "    return destination, filename\n",
    "\n",
    "# Tilt image to certain angles\n",
    "def tiltedImage(imagePath, userName):\n",
    "    p = Augmentor.Pipeline(imagePath)\n",
    "    p.rotate(1, 15, 15)\n",
    "    old_image = io.imread(imagePath)\n",
    "    \n",
    "# Mirror image\n",
    "def mirrorImage(imagePath, userName):\n",
    "    p = Augmentor.Pipeline(imagePath)\n",
    "    p.flip_left_right(probability = 1)\n",
    "    old_image = io.imread(imagePath)\n",
    "\n",
    "# Shearing image\n",
    "def shearImage(imagePath, userName):\n",
    "    p = Augmentor.Pipeline(imagePath)\n",
    "    p.shear(probability = 1, max_shear_left = 15, max_shear_right = 15)\n",
    "    old_image = io.imread(imagePath)\n",
    "\n",
    "# Skewing image\n",
    "def skewedImage(imagePath, userName):\n",
    "    p = Augmentor.Pipeline(imagePath)\n",
    "    p.skew(probability = 1, magnitude = 0.7)\n",
    "    old_image = io.imread(imagePath)\n",
    "\n",
    "# Black and White \n",
    "def bwImage(imagePath, userName):\n",
    "    p = Augmentor.Pipeline(imagePath)\n",
    "    p.black_and_white(probability = 1, threshold = 255)\n",
    "    old_image = io.imread(imagePath)\n",
    "  \n",
    "# Initialize pipeline\n",
    "def usePipeline(imagePath, destination, userName):\n",
    "    p = Augmentor.Pipeline(imagePath, imagePath)\n",
    "    p1 = Augmentor.Pipeline(imagePath, imagePath)\n",
    "    p2 = Augmentor.Pipeline(imagePath, imagePath)\n",
    "    p3 = Augmentor.Pipeline(imagePath, imagePath)\n",
    "    p4 = Augmentor.Pipeline(imagePath, imagePath)\n",
    "    \n",
    "    p.rotate(1, 15, 15)\n",
    "    p1.flip_left_right(probability = 1)\n",
    "    p2.shear(probability = 1, max_shear_left = 15, max_shear_right = 15)\n",
    "    p3.skew(probability = 1, magnitude = 0.7)\n",
    "    p4.black_and_white(probability = 1, threshold = 64)\n",
    "\n",
    "    p.sample(25)\n",
    "    p1.sample(25)\n",
    "    p2.sample(25)\n",
    "    p3.sample(25)\n",
    "    p4.sample(25)\n",
    "\n",
    "    uploadFiles(imagePath, destination, userName)\n",
    "  \n",
    "# Function to upload files to Firebase Storage\n",
    "def uploadFiles(imagePath, destination, userName):\n",
    "    imagePaths = list(pts.list_images(imagePath))\n",
    "    for imgPath in imagePaths:\n",
    "      fileName = imgPath.split('/')[-1]\n",
    "      dest = destination + \"/\" + fileName\n",
    "      storage.child(dest).put(imgPath)\n",
    "\n",
    "# Function to delete collection\n",
    "def deleteCollection():\n",
    "    users_ref = db.collection(u'New Users')\n",
    "    docs = users_ref.stream()\n",
    "\n",
    "    for doc in docs:\n",
    "      doc.reference.delete()\n",
    "\n",
    "# Function to delete /New Datasets folder from Firebase Storage\n",
    "def deleteFolder(folderPaths):\n",
    "    for folderPath in folderPaths:\n",
    "      storage.delete(folderPath)\n",
    "\n",
    "# Function to create new datasets and store them in Firebase Storage\n",
    "def newDatasets(imagePaths, paths):\n",
    "\n",
    "  name = firestoreName()\n",
    "  count = 0\n",
    "\n",
    "  for imagePath in imagePaths:\n",
    "\n",
    "      # Perform data augmentation\n",
    "      \n",
    "      destbl, filebl = blurImage(imagePath, name, count)\n",
    "      destsh, filesh = sharpenImage(imagePath, name, count)\n",
    "      destse, filese = sepiaImage(imagePath, name, count)\n",
    "      destbr, filebr = brightImage(imagePath, name, count)\n",
    "\n",
    "      storage.child(destbl).put(filebl)\n",
    "      storage.child(destsh).put(filesh)\n",
    "      storage.child(destse).put(filese)\n",
    "      storage.child(destbr).put(filebr)\n",
    "\n",
    "      os.remove(filebl)\n",
    "      os.remove(filesh)\n",
    "      os.remove(filese)\n",
    "      os.remove(filebr)\n",
    "\n",
    "      count += 1\n",
    "\n",
    "  imgPath = str(os.getcwd()) + \"/temp\"\n",
    "\n",
    "  imgPaths = []\n",
    "  for j in paths:\n",
    "    i = str(j)\n",
    "    dest = imgPath + \"/\" + str((i.split('/'))[-1])\n",
    "    storage.child(i).download(dest)\n",
    "    imgPaths.append(dest)\n",
    "\n",
    "  destination = 'Datasets/' + name\n",
    "  usePipeline(imgPath, destination, name)\n",
    "\n",
    "  print(\"\\nDone with all images.\\n\")\n",
    "\n",
    "  # destination1 = '/content/drive/MyDrive/Open Lab/Datasets 1/' + name\n",
    "  # movePhotos(destination1, name)\n",
    "  deleteCollection()\n",
    "  print('\\nDeleted new datasets after moving.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TObdcCk9meJd",
    "outputId": "cfa5a4ac-37c7-446d-edb8-dbe45c4913f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['New Datasets/Sayanth/IMG-20210308-WA0297.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0298.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0299.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0300.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0301.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0303.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0304.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0305.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0306.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0307.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0308.jpg', 'New Datasets/Sayanth/IMG-20210308-WA0309.jpg']\n",
      "Press any key to continue:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Executing Pipeline:   0%|          | 0/25 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 12 image(s) found.\n",
      "Output directory set to /content/temp.Initialised with 12 image(s) found.\n",
      "Output directory set to /content/temp.Initialised with 12 image(s) found.\n",
      "Output directory set to /content/temp.Initialised with 12 image(s) found.\n",
      "Output directory set to /content/temp.Initialised with 12 image(s) found.\n",
      "Output directory set to /content/temp."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=958x1280 at 0x7FB9EFDCB690>: 100%|██████████| 25/25 [00:04<00:00,  5.93 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=958x1280 at 0x7FB9ECB8AF90>: 100%|██████████| 25/25 [00:01<00:00, 21.45 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=958x1280 at 0x7FB9ECB8D890>: 100%|██████████| 25/25 [00:04<00:00,  5.98 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=958x1280 at 0x7FB9ECB6F610>: 100%|██████████| 25/25 [00:03<00:00,  6.48 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=1 size=958x1280 at 0x7FB9ECB6F610>: 100%|██████████| 25/25 [00:00<00:00, 28.92 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with all images.\n",
      "\n",
      "\n",
      "Deleted new datasets after moving.\n",
      "\n",
      "New Datasets folder deleted.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "url = \"https://drive.google.com/uc?export=download&id=1vIl_ircPdiLWTkuV2LSTYTJzFOy_cAGB\"\n",
    "r = requests.get(url, allow_redirects = True)\n",
    "# output = 'serviceRequestKey.json'\n",
    "data = r.json()\n",
    "\n",
    "f = open('serviceRequestKey.json', 'w')\n",
    "json.dump(data, f)\n",
    "f.close()\n",
    "\n",
    "config = {\n",
    "    \"apiKey\": \"AIzaSyBqFROlkrLs0fMirkUoV4Sutn8AkTlBPlQ\",\n",
    "    \"authDomain\": \"human-pokedex.firebaseapp.com\",\n",
    "    \"projectId\": \"human-pokedex\",\n",
    "    \"databaseURL\": \"\",\n",
    "    \"storageBucket\": \"human-pokedex.appspot.com\",\n",
    "    \"messagingSenderId\": \"466324270281\",\n",
    "    \"appId\": \"1:466324270281:web:d61e64e5c20932db15b118\",\n",
    "    \"measurementId\": \"G-V0DRGWJ51J\",\n",
    "    \"serviceAccount\": \"serviceRequestKey.json\"\n",
    "}\n",
    "\n",
    "firebase = pyrebase.initialize_app(config)\n",
    "storage = firebase.storage()\n",
    "\n",
    "cred = credentials.Certificate(\"serviceRequestKey.json\")\n",
    "firebase_admin.initialize_app(cred, {'projectId': 'human-pokedex'})\n",
    "db = firestore.client()\n",
    "\n",
    "# List containing paths of images in Firebase Storage (New Datasets/)\n",
    "l = storage.list_files()\n",
    "paths1 = []\n",
    "for i in l:\n",
    "  path = extractPath(i)\n",
    "  if (path.split('/')[0] == \"New Datasets\"):\n",
    "    paths1.append(path)\n",
    "paths2 = paths1[1:]\n",
    "paths2.append(paths2[0])\n",
    "paths2.pop(0)\n",
    "paths1 = paths1[2:]\n",
    "\n",
    "# List of corresponding URLs for all paths in previous list\n",
    "path_urls = []\n",
    "for i in paths1:\n",
    "  url = storage.child(str(i)).get_url(None)\n",
    "  path_urls.append(url)\n",
    "\n",
    "# Perform Data Augmentation and upload new dataset\n",
    "print(paths1)\n",
    "input(\"Press any key to continue:\\n\")\n",
    "imgPath = str(os.getcwd()) + \"/temp\"\n",
    "os.mkdir(imgPath)\n",
    "newDatasets(path_urls, paths1)\n",
    "shutil.rmtree(imgPath)\n",
    "deleteFolder(paths2)\n",
    "print(\"New Datasets folder deleted.\\n\")\n",
    "# Add a line to save photo folder URL to user's Firestore document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyVfPyGcXm23"
   },
   "outputs": [],
   "source": [
    "def newDatasets(imagePaths, paths1):\n",
    "\n",
    "  name = firestoreName()\n",
    "  count = 0\n",
    "\n",
    "  for imagePath in imagePaths:\n",
    "\n",
    "      # Perform data augmentation\n",
    "      \n",
    "      destbl, filebl = blurImage(imagePath, name, count)\n",
    "      destsh, filesh = sharpenImage(imagePath, name, count)\n",
    "      destse, filese = sepiaImage(imagePath, name, count)\n",
    "      destbr, filebr = brightImage(imagePath, name, count)\n",
    "\n",
    "      storage.child(destbl).put(filebl)\n",
    "      storage.child(destsh).put(filesh)\n",
    "      storage.child(destse).put(filese)\n",
    "      storage.child(destbr).put(filebr)\n",
    "\n",
    "      count += 1\n",
    "\n",
    "  imgPath = str(os.getcwd()) + '/temp'\n",
    "\n",
    "  imgPaths = []\n",
    "  for j in paths1:\n",
    "    i = str(j)\n",
    "    dest = imgPath + \"/temp/\" + (str(i.split('/'))[-1])\n",
    "    storage.child(i).download(dest)\n",
    "    imgPaths.append(dest)\n",
    "\n",
    "  destination = 'Datasets/' + name\n",
    "  usePipeline(imgPath, destination, name)\n",
    "\n",
    "  print(\"\\nDone with all images.\\n\")\n",
    "\n",
    "  # destination1 = '/content/drive/MyDrive/Open Lab/Datasets 1/' + name\n",
    "  # movePhotos(destination1, name)\n",
    "  deleteCollection()\n",
    "  print('\\nDeleted new datasets after moving.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEouG7ZI4wxe"
   },
   "source": [
    "# **Function to move output photos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D9E0xcw3G4M1"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def movePhotos(destination, name):\n",
    "  imagePaths = list(paths.list_images(r\"/content/drive/MyDrive/Open Lab/New Datasets 1\"))\n",
    "  print(imagePaths)\n",
    "  input(\"Press any key.\")\n",
    "  for imagePath in imagePaths:\n",
    "    shutil.copy(imagePath, destination)\n",
    "  \n",
    "  print(\"Moved all files.\\n\")\n",
    "  delPath = '/content/drive/MyDrive/Open Lab/New Datasets 1/' + name\n",
    "  shutil.rmtree(delPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_njatv58BG9R"
   },
   "source": [
    "## **Driver Program for Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "neFgMn6vAbZf",
    "outputId": "2e9310e0-dd24-4c5c-e9f7-0142b5d6d8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 21 image(s) found.\n",
      "Output directory set to /content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output.Initialised with 21 image(s) found.\n",
      "Output directory set to /content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output.Initialised with 21 image(s) found.\n",
      "Output directory set to /content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output.Initialised with 21 image(s) found.\n",
      "Output directory set to /content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Executing Pipeline:   0%|          | 0/10 [00:00<?, ? Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 21 image(s) found.\n",
      "Output directory set to /content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing <PIL.Image.Image image mode=RGB size=640x640 at 0x7FF95BF48A10>: 100%|██████████| 10/10 [00:06<00:00,  1.61 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=3264x2448 at 0x7FF95BF68910>: 100%|██████████| 4/4 [00:00<00:00,  4.92 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=1056x1225 at 0x7FF95E1B0A90>: 100%|██████████| 25/25 [00:19<00:00,  1.29 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=RGB size=1070x1456 at 0x7FF958F159D0>: 100%|██████████| 25/25 [00:13<00:00,  1.85 Samples/s]\n",
      "Processing <PIL.Image.Image image mode=1 size=3264x2448 at 0x7FF958F0EE50>: 100%|██████████| 5/5 [00:00<00:00,  5.44 Samples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done with all images.\n",
      "\n",
      "['/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20210207_144652.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/Screenshot_2019-07-21-15-35-52_1_600x600.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20201121_102858.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20201114_204511.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20201114_204455.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20191028-WA0000.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20200904_200538.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20191028_225637.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20190721-WA0006.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20200203-WA0010.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20190614-WA0003.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20190601-WA0015.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG-20181106-WA0013.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201019185236918_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20201203_002234.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/IMG_20191118_173222.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_01a8d56f-6b36-4a28-82dd-6fa20c6157c4.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20181106-WA0013.jpg_73b8d625-88e9-4aa8-b7bd-92c83ad1c8ee.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_33cff644-f826-4738-993c-8eb11fde53de.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190614-WA0003.jpg_99bcd283-7cac-4f06-8ec5-64eec75ff8de.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201114_204511.jpg_e25588c2-0208-4c55-ada2-4b2687416ba0.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg_f4b71525-b3df-4165-81a3-14955c5097c6.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185236918_COVER.jpg_de542b37-66a7-46f2-8ac6-3633d9cffac2.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201203_002234.jpg_aaf61344-3cc0-489b-814f-3da0ad02ea9b.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_ae6bda53-24a6-42ac-845b-f657b3a24ecd.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg_14b57ac9-2af1-4e2c-9dc7-1778e36e3c2e.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_2cbc153d-a47d-44bf-acf9-8735593181dc.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190614-WA0003.jpg_191adbc5-550c-4a7e-975a-737958b21c08.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_0b21a85c-8b67-4520-bfb6-926436c2e1fc.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_fc4164cd-09ee-450c-a11a-26836fe06e65.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_Screenshot_2019-07-21-15-35-52_1_600x600.jpg_2fd8f695-3a92-4d89-a0f1-0a9023766af1.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_Screenshot_2019-07-21-15-35-52_1_600x600.jpg_dc3710ce-aecc-455b-805a-71a216a88735.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_4bd86955-3a05-453b-bc01-fdb7e0fff938.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20191028-WA0000.jpg_202f07cb-0778-48c9-9afa-fab2b128a5f4.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190721-WA0006.jpg_22928f88-97cd-44ed-be66-7c07d9453dc3.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190601-WA0015.jpg_7b9100f3-83c1-4984-9152-992e1f58452a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20200904_200538.jpg_649b9adf-37bf-47cc-bbde-8d63fe5b7f0b.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20200904_200538.jpg_e1d6964c-860c-40eb-a4b8-c6e588114640.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_3623a081-64e1-4e29-b75b-117a28b70eec.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185236918_COVER.jpg_71d43c43-f416-421e-8f0b-96a0bd148ce1.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg_a42692a1-97de-49f8-a060-ab357347178a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_e82f9307-0cff-4802-bc2a-c6acca190a52.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20191118_173222.jpg_f5c39786-b5c8-4ef8-b1d2-819e691d9af4.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_1fc48ef3-eed9-4c2f-a464-c1fe848f0bce.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_d898e595-f4ab-413e-8290-669eef05abeb.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190721-WA0006.jpg_a9abc42c-110a-4ba1-858f-58016f82f0dd.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190614-WA0003.jpg_f934c6d7-6134-4eef-b605-f3aa76b0416e.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201114_204511.jpg_c237ad8b-176e-4d63-8e09-f916f3246c1e.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg_0a1a25dc-47a6-42fd-8480-100c095c1cad.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190601-WA0015.jpg_4d847b10-1991-4487-bb3a-f26f6c982678.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg_abe7129b-d8f5-42e6-a55a-410b738f2d9a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_5f56078a-4e2b-462d-be9a-c1055eac2904.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg_0914f094-4355-4d78-98b2-211348d3ed4a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20191118_173222.jpg_946f7c8c-2d5c-4e1e-b34f-e5d639cd1d00.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_aaa6c51b-57f2-4f24-a4f5-f04d1b7cba33.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_77a0969e-1757-4c1b-ac53-6732c7c31f1c.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190721-WA0006.jpg_8f3a67cb-7f94-4ef1-8510-876b51b80738.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190721-WA0006.jpg_94b483e8-476e-4b52-8d1d-5401d0ce17b0.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20191028_225637.jpg_8e85e296-54ff-40d6-8f64-9d80915160c4.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190614-WA0003.jpg_6bbe5aea-022c-428f-9c9c-bb99bd3f80a8.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201114_204455.jpg_4bc1f9fb-9c25-4d51-af52-265f1b10a8e9.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190614-WA0003.jpg_555dbd89-8a30-4d89-96c8-6abd1b7e588a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20191028_225637.jpg_9ca51a5a-577e-4e25-8428-05cc414253e3.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190601-WA0015.jpg_ff86d7e4-ba29-434c-bdea-4b127e175be9.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_Screenshot_2019-07-21-15-35-52_1_600x600.jpg_2b09890d-1a99-4cbd-ae6a-ba440770594a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20200904_200538.jpg_6049b9ce-d9f2-4b9c-ae5e-b26a4f73395c.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg_7e962a24-c0e8-49d5-883f-c62e58b68ead.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg_25add272-6c8e-4082-b0ae-b8e987356d31.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_366b446e-c716-482c-8848-97353ff01175.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_269525c4-c407-4b35-b537-b219380a14bb.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg_c47694cd-0657-480c-93ec-0af729ee36f3.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg_15f19195-5580-4e00-86cc-eb19a206f8d2.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg_83252a3c-686b-4431-9e65-fed4cb2f26f2.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_f15bc25b-1005-4238-8c96-4061dc558a7c.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_b42270dc-c021-41d9-9ede-7896cdb04252.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201114_204511.jpg_5f65ba39-b80a-450c-9b3d-5b768732111a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg_2d1b7b93-b128-47ea-a285-043e9ff53f2a.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201114_204455.jpg_5b4cf40c-7202-4188-ad1e-7b5f7a09ca4d.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg_e8f4d639-1c1e-443f-bc04-4b03c03e513c.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20190601-WA0015.jpg_967bfdce-fde1-414a-ac1f-eb23338287dc.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20191028-WA0000.jpg_c8cfda46-73e9-4dbe-ad7f-53a3d92431e3.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG-20200203-WA0010.jpg_74b0221b-7307-4bf4-94c1-03cbae3fa7e3.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20200904_200538.jpg_a9e8db0e-f165-4acc-a59a-d74c23c3fe3c.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_IMG_20201203_002234.jpg_616817d1-35a8-4bdf-b642-9010df65ee06.jpg', '/content/drive/MyDrive/Open Lab/New Datasets 1/Yash/output/Yash_original_00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg_a12b66e2-92c0-4219-8f14-4d09ca166e2d.jpg']\n",
      "Press any key.\n",
      "Moved all files.\n",
      "\n",
      "\n",
      "Deleted new datasets after moving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "imagePaths = list(paths.list_images(r\"/content/drive/MyDrive/Open Lab/New Datasets 1\"))\n",
    "# imgPath = list(paths.list_images(r'/content/drive/MyDrive/Open Lab/Datasets/Vikas'))\n",
    "\n",
    "name = firestoreName()\n",
    "os.mkdir(r'/content/drive/MyDrive/Open Lab/Datasets 1/' + name)\n",
    "imgPath = \"/content/drive/MyDrive/Open Lab/New Datasets 1/\" + name\n",
    "count = 0\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "\n",
    "    # Perform data augmentation\n",
    "    \n",
    "    blurImage(imagePath, name, count)\n",
    "    sharpenImage(imagePath, name, count)\n",
    "    sepiaImage(imagePath, name, count)\n",
    "    brightImage(imagePath, name, count)\n",
    "\n",
    "    count += 1\n",
    "    \n",
    "# shearImage(imgPath, name)\n",
    "# skewedImage(imgPath, name)\n",
    "# bwImage(imgPath, name)\n",
    "# tiltedImage(imgPath, name)\n",
    "# mirrorImage(imgPath, name)\n",
    "\n",
    "usePipeline(imgPath, name)\n",
    "\n",
    "print(\"\\nDone with all images.\\n\")\n",
    "\n",
    "destination = '/content/drive/MyDrive/Open Lab/Datasets 1/' + name\n",
    "movePhotos(destination, name)\n",
    "db = firestore.client()\n",
    "db.collection(u'New Users').delete()\n",
    "print('\\nDeleted new datasets after moving.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DDOl9GsCGwit",
    "outputId": "ed680e46-3410-4218-ff47-1bcb46556a64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Blob: human-pokedex.appspot.com, Datasets/>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0297.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0298.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0299.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0300.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0301.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0303.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0304.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0305.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0306.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0307.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0308.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vikas/IMG-20210308-WA0309.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0310.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0311.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0312.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0313.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0314.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0315.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0316.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0317.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0318.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0319.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0320.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0321.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Vrushali/IMG-20210308-WA0322.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201005165132315_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201006092201542_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201006092507541_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201019185155759_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201019185217890_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/00100sPORTRAIT_00100_BURST20201019185236918_COVER.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20181106-WA0013.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20190601-WA0015.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20190614-WA0003.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20190721-WA0006.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20191028-WA0000.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG-20200203-WA0010.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20191028_225637.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20191118_173222.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20200904_200538.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20201114_204455.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20201114_204511.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20201121_102858.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20201203_002234.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/IMG_20210207_144652.jpg>, <Blob: human-pokedex.appspot.com, Datasets/Yash/Screenshot_2019-07-21-15-35-52_1_600x600.jpg>, <Blob: human-pokedex.appspot.com, Embeddings/>, <Blob: human-pokedex.appspot.com, Embeddings/embeddings.pickle>, <Blob: human-pokedex.appspot.com, Functions/>, <Blob: human-pokedex.appspot.com, Functions/augment_data.py>, <Blob: human-pokedex.appspot.com, Functions/extract_embeddings.py>, <Blob: human-pokedex.appspot.com, Functions/recognize_video.py>, <Blob: human-pokedex.appspot.com, Functions/register_faces.py>, <Blob: human-pokedex.appspot.com, Functions/train_model.py>, <Blob: human-pokedex.appspot.com, Trained Models/>, <Blob: human-pokedex.appspot.com, Trained Models/le.pickle>, <Blob: human-pokedex.appspot.com, Trained Models/recognizer.pickle>, <Blob: human-pokedex.appspot.com, face_detection_models/>, <Blob: human-pokedex.appspot.com, face_detection_models/deploy.prototxt>, <Blob: human-pokedex.appspot.com, face_detection_models/openface_nn4.small2.v1.t7>, <Blob: human-pokedex.appspot.com, face_detection_models/res10_300x300_ssd_iter_140000.caffemodel>]\n"
     ]
    }
   ],
   "source": [
    "import pyrebase \n",
    "from imutils import paths\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import storage\n",
    "import gdown\n",
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://drive.google.com/uc?export=download&id=1vIl_ircPdiLWTkuV2LSTYTJzFOy_cAGB\"\n",
    "r = requests.get(url, allow_redirects = True)\n",
    "# output = 'serviceRequestKey.json'\n",
    "data = r.json()\n",
    "\n",
    "f = open('serviceRequestKey.json', 'w')\n",
    "json.dump(data, f)\n",
    "f.close()\n",
    "\n",
    "config = {\n",
    "    \"apiKey\": \"AIzaSyBqFROlkrLs0fMirkUoV4Sutn8AkTlBPlQ\",\n",
    "    \"authDomain\": \"human-pokedex.firebaseapp.com\",\n",
    "    \"projectId\": \"human-pokedex\",\n",
    "    \"databaseURL\": \"\",\n",
    "    \"storageBucket\": \"human-pokedex.appspot.com\",\n",
    "    \"messagingSenderId\": \"466324270281\",\n",
    "    \"appId\": \"1:466324270281:web:d61e64e5c20932db15b118\",\n",
    "    \"measurementId\": \"G-V0DRGWJ51J\",\n",
    "    \"serviceAccount\": \"serviceRequestKey.json\"\n",
    "}\n",
    "\n",
    "firebase = pyrebase.initialize_app(config)\n",
    "storage = firebase.storage()\n",
    "\n",
    "l = storage.list_files()\n",
    "paths = []\n",
    "for i in l:\n",
    "  path = extractPath(i)\n",
    "  if (path.split('/')[0] == \"New Datasets\"):\n",
    "    paths.append()\n",
    "paths = paths[2:]\n",
    "\n",
    "path_urls = []\n",
    "for i in paths:\n",
    "  url = storage.child(str(i)).get_url()\n",
    "  path_urls.append(url)\n",
    "\n",
    "newDatasets(path_urls)\n",
    "# Add a line to save photo folder URL to user's Firestore document\n",
    "\n",
    "# Function to extract path of new datasets uploaded\n",
    "def extractPath(i):\n",
    "  a = (str(i).split(',')[1:])\n",
    "  b = str(a[-1]).rstrip('>')\n",
    "  c = b.lstrip()\n",
    "  return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5taoRp1qtkzs"
   },
   "source": [
    "## **Recognising and tracking students on CCTV footage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s066GPKetkL4"
   },
   "outputs": [],
   "source": [
    "# from imutils.video import VideoStream\n",
    "# from imutils.video import FPS\n",
    "import urllib\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import pickle\n",
    "import time\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LOAIQBgqZSJ"
   },
   "outputs": [],
   "source": [
    "print(\"Loading face detector...\\n\")\n",
    "protoPath = os.path.sep.join([r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Project Files\\opencv-face-recognition\\face_detection_model', 'deploy.prototxt'])\n",
    "modelPath = os.path.sep.join([r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Project Files\\opencv-face-recognition\\face_detection_model', 'res10_300x300_ssd_iter_140000.caffemodel'])\n",
    "detector = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
    "print(\"Loaded face detector.\\n\")\n",
    "\n",
    "# Load our serialized face embedding model\n",
    "print(\"\\nLoading face recognizer...\\n\")\n",
    "embedder = cv2.dnn.readNetFromTorch(r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Project Files\\opencv-face-recognition\\openface_nn4.small2.v1.t7')\n",
    "print(\"Loaded Face Recognizer.\\n\")\n",
    "\n",
    "# Load the SVM Model and LabelEncoder\n",
    "recognizer = pickle.loads(open(r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Project Files\\Trained Models\\recognizer.pickle', \"rb\").read())\n",
    "le = pickle.loads(open(r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Project Files\\Trained Models\\le.pickle', \"rb\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LW-4gAllu4Iw"
   },
   "outputs": [],
   "source": [
    "# Running video from custom camera/ video source over seperate IP Address\n",
    "\n",
    "print(\"Starting video stream:\")\n",
    "\n",
    "# vs = VideoStream(src = 0).start()\n",
    "url = \"\"\n",
    "\n",
    "while True:\n",
    "  imgResp = urllib.urlopen(url)\n",
    "  imgNp = np.array(bytearray(imgResp.read()), dtype = np.uint8)\n",
    "  img = cv2.imdecode(imgNp, cv2.IMREAD_COLOR)\n",
    "  # cv2.imshow('test', img)\n",
    "\n",
    "  img = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 1.0, (300, 300), (104, 177, 123), swapRB = False, crop = False)\n",
    "\n",
    "  detector = setInput(imageBlob)\n",
    "  detections = detector.forward()\n",
    "\n",
    "  # loop over the detections\n",
    "  for i in range(0, detections.shape[2]):\n",
    "    # extract the confidence (i.e., probability) associated with the prediction\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    # filter out weak detections\n",
    "\n",
    "    if confidence > args[\"confidence\"]:\n",
    "      # compute the (x, y) coordinates of the bounding box for the face\n",
    "      box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "      (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "      # extract the face ROI\n",
    "      face = image[startY:endY, startX:endX]\n",
    "      (fH, fW) = face.shape[:2]\n",
    "\n",
    "      # ensure the face width and height are sufficiently large\n",
    "      if ((fW < 20) or (fH < 20)):\n",
    "        continue\n",
    "      \n",
    "      faceBlob = cv2.dnn.blobFromImage(face, 1.0 / 255, (96, 96), (0, 0, 0), swapRB=True, crop=False)\n",
    "      embedder.setInput(faceBlob)\n",
    "      vec = embedder.forward()\n",
    "      # perform classification to recognize the face\n",
    "      preds = recognizer.predict_proba(vec)[0]\n",
    "      j = np.argmax(preds)\n",
    "      proba = preds[j]\n",
    "      name = le.classes_[j]\n",
    "\n",
    "      text = \"{}: {:.2f}%\".format(name, proba * 100)\n",
    "      y = startY - 10 if startY - 10 > 10 else startY + 10\n",
    "      cv2.rectangle(image, (startX, startY), (endX, endY), (0, 0, 255), 2)\n",
    "      cv2.putText(image, text, (startX, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "\n",
    "      cv2.imshow(\"Video\", img)\n",
    "      # cv2.waitKey(0)\n",
    "\n",
    "\n",
    "  if (ord('q') == cv2.waitKey(10)):\n",
    "    exit(0)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Human PokeDex Playground.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
