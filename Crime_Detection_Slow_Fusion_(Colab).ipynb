{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "Crime Detection - Slow Fusion (Colab).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/newb-dev-1008/Human-PokeDex/blob/master/Crime_Detection_Slow_Fusion_(Colab).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x207SioTTsNW"
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "# import the necessary packages\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from imutils import paths\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naNvirn_T8-p",
        "outputId": "b2721944-e68f-41c9-fe3f-d7a3312aef5d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLQCUGU5TsNg",
        "outputId": "054297ac-0936-41d3-e82e-94bc99d48458"
      },
      "source": [
        "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
        "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Datasets\\Annotated'))\n",
        "\n",
        "data = []\n",
        "labels = []\n",
        "clipNos = []\n",
        "\n",
        "init_clipNo = 1\n",
        "clipData = []\n",
        "old_label = \"\"\n",
        "old_clipNo = 0\n",
        "\n",
        "for imagePath in imagePaths:\n",
        "    label = imagePath.split(os.path.sep)[-3]\n",
        "    clipNo = imagePath.split(os.path.sep)[-2]\n",
        "    \n",
        "    if (clipNo != init_clipNo):\n",
        "        init_clipNo = clipNo\n",
        "        data.append(clipData)\n",
        "        labels.append(old_label)\n",
        "        clipNos.append(old_clipNo)\n",
        "        clipData = []\n",
        "        \n",
        "        image = cv2.imread(imagePath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (902, 902))\n",
        "        clipData.append(image)\n",
        "    else:\n",
        "        old_clipNo = clipNo\n",
        "        old_label = label\n",
        "        \n",
        "        image = cv2.imread(imagePath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, (902, 902))\n",
        "        clipData.append(image)\n",
        "    \n",
        "data.append(clipData)\n",
        "labels.append(old_label)\n",
        "clipNos.append(old_clipNo)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-3-3df619966c54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mold_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m902\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m902\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "makrkgbuTsNm",
        "outputId": "791a277b-b2fa-4ff2-8b52-e5a5abb8b196"
      },
      "source": [
        "# Defining the architectures of the first and second layers for temporal convolutions\n",
        "\n",
        "first_layer = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 1, input_shape = (902, 902, 3)),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.AveragePooling2D(pool_size = (2, 2))\n",
        "])\n",
        "\n",
        "second_layer = tf.keras.models.Sequential([\n",
        "    # input_shape needs to be fixed\n",
        "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 1, input_shape = first_layer.output.shape[1:]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.AveragePooling2D(pool_size = (2, 2))\n",
        "])\n",
        "\n",
        "first_layer.summary()\n",
        "second_layer.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 900, 900, 64)      1792      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 900, 900, 64)      256       \n",
            "_________________________________________________________________\n",
            "average_pooling2d (AveragePo (None, 450, 450, 64)      0         \n",
            "=================================================================\n",
            "Total params: 2,048\n",
            "Trainable params: 1,920\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 448, 448, 32)      18464     \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 448, 448, 32)      128       \n",
            "_________________________________________________________________\n",
            "average_pooling2d_1 (Average (None, 224, 224, 32)      0         \n",
            "=================================================================\n",
            "Total params: 18,592\n",
            "Trainable params: 18,528\n",
            "Non-trainable params: 64\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpwhbk57TsNo",
        "outputId": "7d2c9e4a-8213-440e-f814-bd5e4d7a9fab"
      },
      "source": [
        "output = []\n",
        "lb = []\n",
        "flag = 0\n",
        "\n",
        "# Loop for each video clip in the entire dataset\n",
        "for i in range(len(data)):\n",
        "    curr = 0\n",
        "    tenFrames = []\n",
        "    label = labels[i]\n",
        "    \n",
        "    # Loop for each 10 frames in a video clip\n",
        "    while ((curr + 10) < len(data[i])):\n",
        "        lb.append(label)\n",
        "        tenFrames = data[i][curr : (curr + 10)]\n",
        "        \n",
        "        batch_1 = tenFrames[curr : curr + 4]\n",
        "        batch_2 = tenFrames[curr + 2 : curr + 6]\n",
        "        batch_3 = tenFrames[curr + 4 : curr + 8]\n",
        "        batch_4 = tenFrames[curr + 6 : curr + 10]\n",
        "        curr += 10\n",
        "        \n",
        "        # Figure out how to input 4 images at once to first_layer\n",
        "        batch_1_1 = np.stack(np.array(batch_1), axis = 0)\n",
        "        batch_1_2 = np.stack(np.array(batch_2), axis = 0)\n",
        "        batch_1_3 = np.stack(np.array(batch_3), axis = 0)\n",
        "        batch_1_4 = np.stack(np.array(batch_4), axis = 0)\n",
        "        \n",
        "        print(\"Input shape: \", batch_1_1.shape)\n",
        "        \n",
        "        # batch_1_1 = batch_1_1.reshape(4, 224, 224, 3)\n",
        "        # batch_1_2 = batch_1_2.reshape(4, 224, 224, 3)\n",
        "        # batch_1_3 = batch_1_3.reshape(4, 224, 224, 3)\n",
        "        # batch_1_4 = batch_1_4.reshape(4, 224, 224, 3)\n",
        "        \n",
        "        out_1_1 = first_layer(batch_1_1)\n",
        "        out_1_2 = first_layer(batch_1_2)\n",
        "        out_1_3 = first_layer(batch_1_3)\n",
        "        out_1_4 = first_layer(batch_1_4)\n",
        "        \n",
        "        print(\"First layer output shape: \", out_1_1.shape)\n",
        "        \n",
        "        batch_2_1 = np.concatenate([out_1_1, out_1_2])\n",
        "        batch_2_2 = np.concatenate([out_1_3, out_1_4])\n",
        "        \n",
        "        print(\"Batch_2_1 shape: \", batch_2_1.shape)\n",
        "        \n",
        "        out_2_1 = second_layer(batch_2_1)\n",
        "        out_2_2 = second_layer(batch_2_2)\n",
        "        output_map = np.concatenate([out_2_1, out_2_2])\n",
        "        \n",
        "        if (flag == 0):\n",
        "            print(\"Output map dimensions: \", output_map.shape)\n",
        "            flag = 1\n",
        "            \n",
        "        output.append(output_map)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input shape:  (4, 224, 224, 3)\n",
            "First layer output shape:  (4, 31, 31, 64)\n",
            "Batch_2_1 shape:  (8, 31, 31, 64)\n",
            "Output map dimensions:  (16, 4, 4, 32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "need at least one array to stack",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-17-06294c6ebaad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Figure out how to input 4 images at once to first_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mbatch_1_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mbatch_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mbatch_1_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[1;32mc:\\users\\yash umale\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'need at least one array to stack'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWNyJ-AbTsNs"
      },
      "source": [
        "main_data = np.array(output)\n",
        "lbBinarizer = LabelBinarizer()\n",
        "lb = lbBinarizer.fit_transform(lb)\n",
        "\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, lb, test_size = 0.25, stratify = labels, random_state = 42)\n",
        "\n",
        "trainAug = ImageDataGenerator(rotation_range = 30,\n",
        "                             zoom_range = 0.15,\n",
        "                             width_shift_range = 0.2,\n",
        "                             height_shift_range = 0.2,\n",
        "                             shear_range = 0.15,\n",
        "                             horizontal_flip = True,\n",
        "                             fill_mode = \"nearest\")\n",
        "\n",
        "valAug = ImageDataGenerator()\n",
        "\n",
        "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
        "trainAug.mean = mean\n",
        "valAug.mean = mean\n",
        "n_epochs = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQdvwDV_TsNv"
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "    baseModel = ResNet50(weights = \"imagenet\", include_top = False,\n",
        "        input_tensor = Input(shape = output_map.shape[1:]))\n",
        "\n",
        "    headModel = baseModel.output\n",
        "    headModel = AveragePooling2D(pool_size = (7, 7))(headModel)\n",
        "    headModel = Flatten(name = \"flatten\")(headModel)\n",
        "    headModel = Dense(512, activation = \"relu\")(headModel)\n",
        "    headModel = Dropout(0.5)(headModel)\n",
        "    headModel = Dense(len(lb.classes_), activation = \"softmax\")(headModel)\n",
        "\n",
        "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "    for layer in baseModel.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    print(\"Compiling model:\")\n",
        "    opt = SGD(lr = 1e-4, momentum = 0.9, decay = (1e-4 / n_epochs))\n",
        "    model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
        "\n",
        "    print(\"Training head:\")\n",
        "    H = model.fit(\n",
        "        x = trainAug.flow(trainX, trainY, batch_size = 32),\n",
        "        steps_per_epoch = len(trainX) // 32,\n",
        "        validation_data = valAug.flow(testX, testY),\n",
        "        validation_steps = len(testX) // 32,\n",
        "        epochs = n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gBpsRa3TsNx"
      },
      "source": [
        "# Evaluate the network\n",
        "print(\"Evaluating network:\")\n",
        "predictions = model.predict(x = testX.astype(\"float32\"), batch_size = 32)\n",
        "\n",
        "# To print precision, recall, F1 score\n",
        "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = lb.classes_))\n",
        "\n",
        "# Plot the training loss and accuracy\n",
        "N = n_epochs\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/ Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "plotPath = r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Plots'\n",
        "plt.savefig(plotPath)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}